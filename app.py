import os
import time
import json
import hashlib
import requests
import jwt  # from PyJWT
import pandas as pd
import streamlit as st

# ----------------------------
# Page
# ----------------------------
st.set_page_config(page_title="PassKit é‡è¤‡ ID æœå°‹ / å›æ”¶åˆ†é…å·¥å…·", page_icon="â™»ï¸", layout="wide")
st.title("â™»ï¸ PassKit é‡è¤‡ ID æœå°‹ / å›æ”¶åˆ†é…å·¥å…·")
st.caption("1) ç”¨ displayName æŸ¥è©¢ memberId 2) æ‰¾é‡è¤‡/æœªæ‰¾åˆ° 3) å›æ”¶ PASS_ISSUED ä¸” meta_cardNumber ç‚ºç©ºçš„èˆŠ memberId åˆ†é…çµ¦æœªæ‰¾åˆ°åå–®ï¼ˆå…ˆ Dry-run å† Applyï¼‰")

# ----------------------------
# Config helpers
# ----------------------------
def get_config(key: str, default: str | None = None) -> str | None:
    val = st.secrets.get(key) if hasattr(st, "secrets") else None
    if val is None:
        val = os.environ.get(key, default)
    if val is None:
        return None
    return str(val).replace("\\n", "\n").strip()

PK_API_KEY = get_config("PK_API_KEY")
PK_API_SECRET = get_config("PK_API_SECRET")
PK_API_PREFIX = get_config("PK_API_PREFIX", "https://api.pub1.passkit.io")
PROGRAM_ID = get_config("PROGRAM_ID")

missing_cfg = [k for k, v in {
    "PK_API_KEY": PK_API_KEY,
    "PK_API_SECRET": PK_API_SECRET,
    "PK_API_PREFIX": PK_API_PREFIX,
    "PROGRAM_ID": PROGRAM_ID
}.items() if not v]

if missing_cfg:
    st.error(f"âŒ ç¼ºå°‘è¨­å®šï¼š{', '.join(missing_cfg)}ï¼ˆè«‹åœ¨ .env æˆ– Secrets è£œä¸Šï¼‰")
    st.stop()

# ----------------------------
# JWT auth (PassKit style)
# ----------------------------
def make_jwt_for_body(body_text: str) -> str:
    now = int(time.time())
    payload = {
        "uid": PK_API_KEY,
        "iat": now,
        "exp": now + 600,
    }
    if body_text:
        payload["signature"] = hashlib.sha256(body_text.encode("utf-8")).hexdigest()

    token = jwt.encode(payload, PK_API_SECRET, algorithm="HS256")
    if isinstance(token, bytes):
        token = token.decode("utf-8")
    return token

def _handle_resp_errors(resp: requests.Response) -> None:
    if resp.status_code == 404:
        raise RuntimeError("404 Not Foundï¼šå¤šåŠæ˜¯ API Prefixï¼ˆpub1/pub2ï¼‰æˆ– endpoint path ç”¨éŒ¯ã€‚")
    if resp.status_code in (401, 403):
        raise RuntimeError(f"Auth å¤±æ•—ï¼ˆ{resp.status_code}ï¼‰ï¼šè«‹ç¢ºèª PK_API_KEY/PK_API_SECRETã€ä»¥åŠ API Prefixï¼ˆpub1/pub2ï¼‰ã€‚")
    if not resp.ok:
        raise RuntimeError(f"HTTP {resp.status_code}: {resp.text[:800]}")

def post_list_members(filters_payload: dict) -> list[dict]:
    """
    POST {PK_API_PREFIX}/members/member/list/{PROGRAM_ID}
    PassKit list APIs sometimes return NDJSON (one JSON per line)
    """
    url = f"{PK_API_PREFIX.rstrip('/')}/members/member/list/{PROGRAM_ID}"
    body_text = json.dumps({"filters": filters_payload}, separators=(",", ":"), ensure_ascii=False)

    token = make_jwt_for_body(body_text)
    headers = {
        "Authorization": token,  # PassKit examples: token directly, not Bearer
        "Content-Type": "application/json",
    }

    resp = requests.post(url, headers=headers, data=body_text, timeout=30)
    _handle_resp_errors(resp)

    text = (resp.text or "").strip()
    if not text:
        return []

    items: list[dict] = []
    # Try NDJSON first
    lines = [ln for ln in text.split("\n") if ln.strip()]
    for ln in lines:
        try:
            items.append(json.loads(ln))
        except json.JSONDecodeError:
            # maybe it's a single JSON
            items = [json.loads(text)]
            break
    return items

def put_update_member(payload: dict) -> dict:
    """
    PUT {PK_API_PREFIX}/members/member
    payload includes: programId, id, person/meta updates
    """
    url = f"{PK_API_PREFIX.rstrip('/')}/members/member"
    body_text = json.dumps(payload, separators=(",", ":"), ensure_ascii=False)

    token = make_jwt_for_body(body_text)
    headers = {
        "Authorization": token,
        "Content-Type": "application/json",
    }

    resp = requests.put(url, headers=headers, data=body_text, timeout=30)
    _handle_resp_errors(resp)

    try:
        return resp.json()
    except Exception:
        return {"ok": True, "text": resp.text[:800]}

# ----------------------------
# Helpers
# ----------------------------
def normalize_name(name: str) -> str:
    # ä½ èªª displayName å›ºå®šå…¨å¤§å¯« first+lastã€ç„¡ç©ºæ ¼ï¼›é€™è£¡åªåšåŸºæœ¬ trim
    return (name or "").strip()

def extract_member_obj(item: dict) -> dict | None:
    member = item.get("result") or item.get("member") or item
    return member if isinstance(member, dict) else None

def is_blank_card_number(v) -> bool:
    if v is None:
        return True
    s = str(v).strip()
    return (s == "") or (s.upper() == "NULL")

def extract_member_rows(list_response_items: list[dict], search_name: str, max_hits: int) -> list[dict]:
    """
    Extract: person.displayName, id, passStatus, meta.cardNumber, created/updated(è‹¥æœ‰)
    """
    rows = []
    for item in list_response_items:
        member = extract_member_obj(item)
        if not member:
            continue

        person = member.get("person") or {}
        meta = member.get("meta") or {}  # âœ… ä½ èªªçš„å®¹å™¨ keyï¼šmeta
        if not isinstance(meta, dict):
            meta = {}

        display_name = (person.get("displayName") or "").strip()
        member_id = (member.get("id") or "").strip()
        pass_status = (member.get("passStatus") or "").strip()

        meta_card_number = meta.get("cardNumber")
        meta_card_number = "" if meta_card_number is None else str(meta_card_number).strip()

        created = member.get("created") or member.get("createdAt") or member.get("createdOn")
        updated = member.get("updated") or member.get("updatedAt") or member.get("updatedOn")

        if display_name and member_id:
            rows.append({
                "æœå°‹å§“å": search_name,
                "displayName": display_name,
                "memberId": member_id,
                "passStatus": pass_status,
                "cardNumber": card_number,
                "created": str(created) if created is not None else "",
                "updated": str(updated) if updated is not None else "",
            })

        if len(rows) >= max_hits:
            break
    return rows

def search_by_display_name(name: str, max_hits: int, operator: str) -> list[dict]:
    filters = {
        "limit": min(int(max_hits), 1000),
        "offset": 0,
        "filterGroups": [{
            "condition": "AND",
            "fieldFilters": [{
                "filterField": "displayName",
                "filterValue": name,
                "filterOperator": operator,  # "eq" or "like"
            }]
        }]
    }
    items = post_list_members(filters)
    return extract_member_rows(items, name, max_hits=max_hits)

def list_recycle_pool_issued_cardnumber_null(limit: int = 300, offset: int = 0) -> list[dict]:
    """
    å…¨åŸŸå›æ”¶æ± ï¼ˆå¯é¸ï¼‰ï¼š
    PASS_ISSUED ä¸” cardNumber == NULL çš„ memberId
    - æœ‰äº› PassKit å¾Œç«¯æœƒç”¨ "NULL" ç•¶ä½œ null filterValue
    - é€™è£¡ä¹ŸåšäºŒæ¬¡æª¢æŸ¥ï¼šå›ä¾†å¾Œå†ç”¨ is_blank_card_number() éæ¿¾
    """
    filters = {
        "limit": min(int(limit), 1000),
        "offset": int(offset),
        "filterGroups": [{
            "condition": "AND",
            "fieldFilters": [
                {"filterField": "passStatus", "filterValue": "PASS_ISSUED", "filterOperator": "eq"},
                {"filterField": "cardNumber", "filterValue": "NULL", "filterOperator": "eq"},
            ]
        }],
        "orderBy": "created",
        "orderAsc": True,
    }

    items = post_list_members(filters)
    pool = []
    for item in items:
        member = extract_member_obj(item)
        if not member:
            continue
        meta = member.get("meta") or {}
        if not isinstance(meta, dict):
            meta = {}

        mid = (member.get("id") or "").strip()
        ps = (member.get("passStatus") or "").strip()
        mcn = meta.get("cardNumber")

        if mid and ps == "PASS_ISSUED" and is_blank_card_number(mcn):
            pool.append({
                "memberId": mid,
                "passStatus": ps,
                "cardNumber": "" if mcn is None else str(mcn).strip(),
                "created": str(member.get("created") or ""),
            })
    return pool

def choose_duplicate_recycle_candidates(df_hits: pd.DataFrame) -> pd.DataFrame:
    """
    å¾åŒåå¤šç­†ä¸­æŒ‘å›æ”¶å€™é¸ï¼š
    - æ¯å€‹ displayName ä¿ç•™ 1 ç­†ï¼ˆç•¶æˆæœ€æ–°/ä¸»ç­†ï¼‰
    - å…¶é¤˜è‹¥ passStatus=PASS_ISSUED ä¸” meta_cardNumber ç©º â†’ å›æ”¶æ± 
    """
    if df_hits.empty:
        return df_hits.iloc[0:0].copy()

    work = df_hits.copy()

    # created/updated è‹¥å¯è§£æå°±æ’åºæ›´ç©©
    for col in ["updated", "created"]:
        if col in work.columns:
            work[col] = pd.to_datetime(work[col], errors="coerce")

    candidates = []
    for name, g in work.groupby("displayName", dropna=False):
        if len(g) <= 1:
            continue

        # newest firstï¼ˆupdated > createdï¼‰
        if g["updated"].notna().any() or g["created"].notna().any():
            g_sorted = g.sort_values(["updated", "created"], ascending=[False, False], na_position="last")
        else:
            g_sorted = g.copy()

        # keep newest (first row)
        rest = g_sorted.iloc[1:]

        for _, r in rest.iterrows():
            if (r.get("passStatus") == "PASS_ISSUED") and is_blank_card_number(r.get("cardNumber")):
                candidates.append(r.to_dict())

    return pd.DataFrame(candidates) if candidates else work.iloc[0:0].copy()

def build_put_payload_reassign(member_id: str, new_display_name: str) -> dict:
    """
    å›æ”¶åˆ†é…æ™‚ï¼š
    - æ›´æ–° person.displayName
    - åŒæ™‚å¯«å…¥ä½”ä½ meta_cardNumberï¼Œé¿å…ä¸‹ä¸€æ¬¡åˆè¢«ç•¶æˆå¯å›æ”¶
    """
    new_display_name = normalize_name(new_display_name)
    return {
        "programId": PROGRAM_ID,
        "id": member_id,
        "person": {"displayName": new_display_name},
        "meta": {"cardNumber": f"TEMP_{member_id}"},
    }

# ----------------------------
# UI
# ----------------------------
with st.form("search_form"):
    input_text = st.text_area(
        "æ¯è¡Œä¸€å€‹ full nameï¼ˆPassKit: person.displayNameï¼‰â€” æœ€å¤š 150 è¡Œ",
        height=220,
        placeholder="MEIHUA LEE\nHSIUTING CHOU\nKUANYEN LEE\n..."
    )

    colA, colB, colC, colD = st.columns([1, 1, 1, 2])
    with colA:
        max_hits = st.number_input("åŒåæœ€å¤šå›å‚³ç­†æ•¸", min_value=1, max_value=150, value=10, step=1)
    with colB:
        operator = st.selectbox("æ¯”å°æ–¹å¼", options=["eq", "like"], index=0)
    with colC:
        throttle = st.number_input("æ¯æ¬¡ API é–“éš”ç§’æ•¸", min_value=0.0, max_value=2.0, value=0.15, step=0.05)
    with colD:
        st.caption("eq = å®Œå…¨ç›¸åŒï¼›like = åŒ…å«ï¼ˆè¼ƒé¬†ï¼Œå¯èƒ½æœƒå›æ›´å¤šçµæœï¼‰")

    submitted = st.form_submit_button("Search")

if submitted:
    names = [normalize_name(n) for n in (input_text or "").splitlines() if normalize_name(n)]
    if not names:
        st.warning("è«‹å…ˆè²¼ä¸Šè‡³å°‘ä¸€è¡Œå§“åã€‚")
        st.stop()

    if len(names) > 150:
        st.warning(f"ä½ è²¼äº† {len(names)} è¡Œï¼Œç³»çµ±åªæœƒå–å‰ 150 è¡Œã€‚")
        names = names[:150]

    all_rows = []
    missing = []

    prog = st.progress(0.0)
    status = st.empty()

    for i, name in enumerate(names, start=1):
        status.info(f"æŸ¥è©¢ä¸­ {i}/{len(names)}ï¼š{name}")
        try:
            rows = search_by_display_name(name, max_hits=int(max_hits), operator=operator)
            if rows:
                all_rows.extend(rows)
            else:
                missing.append(name)
        except Exception as e:
            st.error(f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{name} â†’ {e}")
            missing.append(name)

        prog.progress(i / len(names))
        if float(throttle) > 0:
            time.sleep(float(throttle))

    status.empty()
    prog.empty()

    st.session_state["hits_rows"] = all_rows
    st.session_state["missing_names"] = missing

    st.success(f"å®Œæˆï¼šæŸ¥è©¢ {len(names)} ç­†ï¼Œå‘½ä¸­ {len(all_rows)} ç­†ï¼›æœªæ‰¾åˆ° {len(missing)} ç­†ã€‚")

# ----------------------------
# Render results
# ----------------------------
hits_rows = st.session_state.get("hits_rows") or []
missing_names = st.session_state.get("missing_names") or []

if hits_rows:
    df_hits = pd.DataFrame(hits_rows)
    cols_order = [c for c in ["æœå°‹å§“å", "displayName", "memberId", "passStatus", "meta_cardNumber", "created", "updated"] if c in df_hits.columns]
    df_hits = df_hits[cols_order].copy()

    left, right = st.columns([2, 1], gap="large")
    with left:
        st.subheader("å‘½ä¸­æ¸…å–®")
        st.dataframe(df_hits, use_container_width=True, height=420)
        csv = df_hits.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ä¸‹è¼‰å‘½ä¸­ CSV", data=csv, file_name="passkit_member_hits.csv", mime="text/csv")

    with right:
        st.subheader("é‡è¤‡çµ±è¨ˆï¼ˆæŒ‰ displayNameï¼‰")
        dup_counts = (
            df_hits.groupby("displayName")["memberId"]
            .nunique()
            .reset_index(name="åŒå memberId æ•¸é‡")
            .sort_values("åŒå memberId æ•¸é‡", ascending=False)
        )
        dup_only = dup_counts[dup_counts["åŒå memberId æ•¸é‡"] > 1].copy()
        st.metric("åŒåé‡è¤‡åç¨±æ•¸", int(len(dup_only)))
        st.dataframe(dup_only, use_container_width=True, height=260)

        if missing_names:
            st.subheader("æœªæ‰¾åˆ°åå–®ï¼ˆmissingï¼‰")
            st.write("\n".join(missing_names))
        else:
            st.info("æ²’æœ‰ missingã€‚")

elif submitted:
    st.info("æ²’æœ‰å‘½ä¸­è³‡æ–™ï¼ˆhits ç‚º 0ï¼‰ã€‚è‹¥ä½ ç¢ºèªè³‡æ–™å­˜åœ¨ï¼Œè«‹æª¢æŸ¥ PROGRAM_ID / API Prefix / operatorã€‚")

# ----------------------------
# Recycle & assign
# ----------------------------
st.divider()
st.header("â™»ï¸ å›æ”¶æ±  â†’ åˆ†é…çµ¦ missingï¼ˆæ¢ä»¶ï¼šPASS_ISSUED + meta.cardNumber ç‚ºç©ºï¼‰")

if not missing_names:
    st.info("ç›®å‰æ²’æœ‰ missing åå–®ï¼Œå› æ­¤ä¸éœ€è¦åˆ†é…å›æ”¶æ± ã€‚")
else:
    st.warning(
        "âš ï¸ é‡è¦æé†’ï¼šä½ ç›®å‰æ²’æœ‰ã€Pass URL æ˜¯å¦æ›¾ç™¼é€/å¤–æµã€çš„ç´€éŒ„ã€‚\n\n"
        "PASS_ISSUED ä»£è¡¨ URL å·²å­˜åœ¨ï¼›å³ä½¿æœª installedï¼Œè‹¥ URL æ›¾å¤–æµï¼Œä½ æŠŠ memberId æ”¹åçµ¦åˆ¥äººï¼Œç­‰æ–¼è½‰æ‰‹ã€‚\n"
        "ä½ è¦æ±‚çš„æ˜¯éæ¸¡æœŸæ¸›è¼•äººå·¥æª¢æŸ¥ï¼Œæ‰€ä»¥æ­¤å·¥å…·ç”¨ã€PASS_ISSUED + meta_cardNumber ç©ºã€åšä¿å®ˆå›æ”¶ã€‚"
    )

    mode = st.radio(
        "å›æ”¶æ± ä¾†æº",
        options=[
            "A) å…¨åŸŸå›æ”¶æ± ï¼šPASS_ISSUED + meta_cardNumber ç‚ºç©ºï¼ˆä¸ä¾è³´é‡è¤‡æŸ¥è©¢ï¼‰",
            "B) åŒåé‡è¤‡å›æ”¶ï¼šæ¯å€‹ displayName ä¿ç•™æœ€æ–° 1 ç­†ï¼Œå…¶é¤˜ç¬¦åˆæ¢ä»¶è€…å›æ”¶ï¼ˆæ›´è²¼è¿‘ä½ æˆªåœ–æƒ…å¢ƒï¼‰",
        ],
        index=1
    )

    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        assign_limit = st.number_input("æœ€å¤šåˆ†é…ç­†æ•¸", min_value=1, max_value=5000, value=min(300, len(missing_names)), step=10)
    with col2:
        apply_throttle = st.number_input("æ¯æ¬¡ PUT é–“éš”ç§’æ•¸", min_value=0.0, max_value=2.0, value=0.2, step=0.05)
    with col3:
        st.caption("æµç¨‹ï¼šå…ˆ Dry-run ç”¢ç”Ÿ mapping â†’ å‹¾é¸ç¢ºèª â†’ Apply æ‰¹æ¬¡ PUTã€‚")

    recycle_ids: list[str] = []

    if mode.startswith("A)"):
        st.subheader("A) å–å¾—å…¨åŸŸå›æ”¶æ± ")
        pool_limit = st.number_input("å›æ”¶æ± æ’ˆå–ä¸Šé™", min_value=10, max_value=1000, value=300, step=50)
        fetch_pool = st.button("å–å¾—å›æ”¶æ± ï¼ˆAï¼‰", type="secondary")
        if fetch_pool:
            try:
                pool = list_recycle_pool_issued_cardnumber_null(limit=int(pool_limit), offset=0)
                st.session_state["recycle_pool_A"] = pool
                st.success(f"å›æ”¶æ± å–å¾—å®Œæˆï¼š{len(pool)} ç­†ã€‚")
            except Exception as e:
                st.error(f"âŒ å–å¾—å›æ”¶æ± å¤±æ•—ï¼š{e}")

        pool = st.session_state.get("recycle_pool_A") or []
        if pool:
            df_pool = pd.DataFrame(pool)
            st.dataframe(df_pool, use_container_width=True, height=260)
            recycle_ids = [x["memberId"] for x in pool if x.get("memberId")]
        else:
            st.info("å°šæœªå–å¾—å›æ”¶æ± ï¼Œæˆ–å›æ”¶æ± ç‚ºç©ºã€‚")

    else:
        st.subheader("B) å¾åŒåé‡è¤‡ä¸­æŒ‘å›æ”¶å€™é¸ï¼ˆä¿ç•™æœ€æ–° 1 ç­†ï¼Œå…¶é¤˜ PASS_ISSUED + meta_cardNumber ç©ºè€…å›æ”¶ï¼‰")
        if not hits_rows:
            st.info("ä½ éœ€è¦å…ˆ Search å–å¾—å‘½ä¸­è³‡æ–™ï¼Œæ‰èƒ½ä½¿ç”¨ B æ¨¡å¼ã€‚")
        else:
            df_hits = pd.DataFrame(hits_rows)
            cand_df = choose_duplicate_recycle_candidates(df_hits)
            if cand_df.empty:
                st.info("æ‰¾ä¸åˆ°ç¬¦åˆå›æ”¶æ¢ä»¶çš„åŒåå€™é¸ï¼ˆPASS_ISSUED + meta_cardNumber ç©ºï¼‰ã€‚")
            else:
                st.success(f"æ‰¾åˆ°å¯å›æ”¶å€™é¸ï¼š{len(cand_df)} ç­†ã€‚")
                show_cols = [c for c in ["displayName", "memberId", "passStatus", "meta_cardNumber", "created", "updated"] if c in cand_df.columns]
                st.dataframe(cand_df[show_cols], use_container_width=True, height=260)
                recycle_ids = [str(x).strip() for x in cand_df["memberId"].tolist() if str(x).strip()]

    # dedupe keep order
    seen = set()
    recycle_ids = [x for x in recycle_ids if not (x in seen or seen.add(x))]

    st.subheader("ğŸ“Œ Dry-runï¼šç”¢ç”Ÿåˆ†é… mappingï¼ˆä¸æœƒå¯«å…¥ï¼‰")
    max_assign = int(min(assign_limit, len(missing_names), len(recycle_ids)))
    if max_assign <= 0:
        st.info("ç›®å‰ç„¡æ³•ç”¢ç”Ÿ mappingï¼šå¯èƒ½æ˜¯å›æ”¶æ± ç‚ºç©ºï¼Œæˆ– missing ç‚ºç©ºã€‚")
    else:
        mapping = [{"new_displayName": missing_names[i], "recycled_memberId": recycle_ids[i]} for i in range(max_assign)]
        df_map = pd.DataFrame(mapping)
        st.dataframe(df_map, use_container_width=True, height=260)

        st.download_button(
            "ä¸‹è¼‰ mapping CSVï¼ˆDry-runï¼‰",
            data=df_map.to_csv(index=False).encode("utf-8-sig"),
            file_name="recycle_mapping_dryrun.csv",
            mime="text/csv"
        )

        st.divider()
        st.subheader("âœ… Applyï¼šä¾ mapping æ‰¹æ¬¡æ›´æ–°ï¼ˆPUT /members/memberï¼‰")

        ack = st.checkbox(
            "æˆ‘äº†è§£é¢¨éšªï¼šPASS_ISSUED ä»å¯èƒ½å·²è¢«åˆ†äº«ã€‚è‹¥å›æ”¶çš„ memberId/URL æ›¾å¤–æµï¼Œæ›´æ–°å¾Œæœƒè®“èˆŠ URL æŒ‡å‘æ–°æœƒå“¡è³‡æ–™ï¼ˆç­‰æ–¼è½‰æ‰‹ï¼‰ã€‚",
            value=False
        )
        do_apply = st.button("Apply æ‰¹æ¬¡æ›´æ–°", type="primary", disabled=not ack)

        if do_apply:
            ok_rows = []
            fail_rows = []

            prog2 = st.progress(0.0)
            status2 = st.empty()

            for i, row in enumerate(mapping, start=1):
                new_name = row["new_displayName"]
                member_id = row["recycled_memberId"]

                status2.info(f"æ›´æ–°ä¸­ {i}/{len(mapping)}ï¼š{member_id} â†’ {new_name}")

                try:
                    payload = build_put_payload_reassign(member_id, new_name)
                    resp = put_update_member(payload)
                    ok_rows.append({
                        "memberId": member_id,
                        "new_displayName": new_name,
                        "result": "OK",
                        "resp": str(resp)[:500],
                    })
                except Exception as e:
                    fail_rows.append({
                        "memberId": member_id,
                        "new_displayName": new_name,
                        "result": "FAIL",
                        "error": str(e)[:800],
                    })

                prog2.progress(i / len(mapping))
                if float(apply_throttle) > 0:
                    time.sleep(float(apply_throttle))

            status2.empty()
            prog2.empty()

            st.success(f"å®Œæˆï¼šæˆåŠŸ {len(ok_rows)} ç­†ï¼›å¤±æ•— {len(fail_rows)} ç­†ã€‚")

            if ok_rows:
                df_ok = pd.DataFrame(ok_rows)
                st.subheader("æˆåŠŸæ¸…å–®")
                st.dataframe(df_ok, use_container_width=True, height=260)
                st.download_button(
                    "ä¸‹è¼‰æˆåŠŸ CSV",
                    data=df_ok.to_csv(index=False).encode("utf-8-sig"),
                    file_name="recycle_apply_success.csv",
                    mime="text/csv"
                )

            if fail_rows:
                df_fail = pd.DataFrame(fail_rows)
                st.subheader("å¤±æ•—æ¸…å–®")
                st.dataframe(df_fail, use_container_width=True, height=260)
                st.download_button(
                    "ä¸‹è¼‰å¤±æ•— CSV",
                    data=df_fail.to_csv(index=False).encode("utf-8-sig"),
                    file_name="recycle_apply_failed.csv",
                    mime="text/csv"
                )
