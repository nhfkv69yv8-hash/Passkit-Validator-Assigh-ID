import os
import time
import json
import hashlib
import requests
import jwt  # PyJWT
import pandas as pd
import streamlit as st
from typing import Any

# ----------------------------
# Page
# ----------------------------
st.set_page_config(page_title="PassKit é‡è¤‡ ID æœå°‹ / å›æ”¶åˆ†é…å·¥å…·", page_icon="ğŸ”", layout="wide")
st.title("ğŸ”â™»ï¸ PassKit é‡è¤‡ ID æœå°‹ / å›æ”¶åˆ†é…å·¥å…·")
st.caption("â‘  è²¼ displayName æ‰¹æ¬¡æŸ¥è©¢ï¼ˆREST Filterï¼‰ã€‚â‘¡ ç”¢ç”Ÿé‡è¤‡/ç¼ºæ¼ã€‚â‘¢ å¯é¸æ“‡å›æ”¶æ±  â†’ è‡ªå‹•åˆ†é…çµ¦ missingï¼ˆå…ˆé è¦½ï¼Œå†å¥—ç”¨ï¼‰ã€‚")

# ----------------------------
# Config helpers
# ----------------------------
def get_config(key: str, default: str | None = None) -> str | None:
    val = st.secrets.get(key) if hasattr(st, "secrets") else None
    if val is None:
        val = os.environ.get(key, default)
    if val is None:
        return None
    return str(val).replace("\\n", "\n").strip()

PK_API_KEY = get_config("PK_API_KEY")
PK_API_SECRET = get_config("PK_API_SECRET")
PK_API_PREFIX = get_config("PK_API_PREFIX", "https://api.pub1.passkit.io")
PROGRAM_ID = get_config("PROGRAM_ID")

missing_cfg = [k for k, v in {
    "PK_API_KEY": PK_API_KEY,
    "PK_API_SECRET": PK_API_SECRET,
    "PK_API_PREFIX": PK_API_PREFIX,
    "PROGRAM_ID": PROGRAM_ID
}.items() if not v]

if missing_cfg:
    st.error(f"âŒ ç¼ºå°‘è¨­å®šï¼š{', '.join(missing_cfg)}ï¼ˆè«‹åœ¨ .env æˆ– Streamlit Secrets è£œä¸Šï¼‰")
    st.stop()

# ----------------------------
# JWT auth (PassKit style)
# ----------------------------
def make_jwt_for_body(body_text: str) -> str:
    now = int(time.time())
    payload = {
        "uid": PK_API_KEY,
        "iat": now,
        "exp": now + 600,  # 10 minutes
    }
    if body_text:
        payload["signature"] = hashlib.sha256(body_text.encode("utf-8")).hexdigest()

    token = jwt.encode(payload, PK_API_SECRET, algorithm="HS256")
    if isinstance(token, bytes):
        token = token.decode("utf-8")
    return token

def _handle_resp_errors(resp: requests.Response) -> None:
    if resp.status_code == 404:
        raise RuntimeError("404 Not Foundï¼šå¤šåŠæ˜¯ API Prefixï¼ˆpub1/pub2ï¼‰æˆ– endpoint path ç”¨éŒ¯ã€‚")
    if resp.status_code in (401, 403):
        raise RuntimeError(f"Auth å¤±æ•—ï¼ˆ{resp.status_code}ï¼‰ï¼šè«‹ç¢ºèª PK_API_KEY/PK_API_SECRETã€ä»¥åŠ API Prefixï¼ˆpub1/pub2ï¼‰ã€‚")
    if not resp.ok:
        raise RuntimeError(f"HTTP {resp.status_code}: {resp.text[:800]}")

def post_list_members(filters_payload: dict) -> list[dict]:
    """
    POST {PK_API_PREFIX}/members/member/list/{PROGRAM_ID}
    PassKit list APIs sometimes return NDJSON (one JSON per line)
    """
    url = f"{PK_API_PREFIX.rstrip('/')}/members/member/list/{PROGRAM_ID}"
    body_text = json.dumps({"filters": filters_payload}, separators=(",", ":"), ensure_ascii=False)

    token = make_jwt_for_body(body_text)
    headers = {
        "Authorization": token,
        "Content-Type": "application/json",
    }

    resp = requests.post(url, headers=headers, data=body_text, timeout=30)
    _handle_resp_errors(resp)

    text = (resp.text or "").strip()
    if not text:
        return []

    items: list[dict] = []
    lines = [ln for ln in text.split("\n") if ln.strip()]
    for ln in lines:
        try:
            items.append(json.loads(ln))
        except json.JSONDecodeError:
            # fallback to single JSON
            items = [json.loads(text)]
            break
    return items

def put_update_member(member_id: str, payload: dict) -> dict:
    """
    PUT {PK_API_PREFIX}/members/member
    payload must include at least: programId, id
    """
    url = f"{PK_API_PREFIX.rstrip('/')}/members/member"
    body_text = json.dumps(payload, separators=(",", ":"), ensure_ascii=False)

    token = make_jwt_for_body(body_text)
    headers = {
        "Authorization": token,
        "Content-Type": "application/json",
    }

    resp = requests.put(url, headers=headers, data=body_text, timeout=30)
    _handle_resp_errors(resp)

    try:
        return resp.json()
    except Exception:
        return {"ok": True, "text": resp.text[:800]}

# ----------------------------
# Helpers: normalize / extract
# ----------------------------
def normalize_name(name: str) -> str:
    # ä¾ä½ èªªçš„è¦æ ¼ï¼šå…¨å¤§å¯« + ä¸­é–“ç„¡ç©ºæ ¼
    return (name or "").strip().upper().replace(" ", "")

def _pick_first_present(d: dict, keys: list[str]) -> Any:
    for k in keys:
        if k in d and d.get(k) is not None:
            return d.get(k)
    return None

def extract_member_obj(item: dict) -> dict | None:
    member = item.get("result") or item.get("member") or item
    return member if isinstance(member, dict) else None

def extract_member_rows(list_response_items: list[dict], search_name: str, max_hits: int) -> list[dict]:
    rows = []
    for item in list_response_items:
        member = extract_member_obj(item)
        if not member:
            continue

        person = member.get("person") or {}
        display_name = (person.get("displayName") or "").strip()
        member_id = (member.get("id") or "").strip()
        pass_status = (member.get("passStatus") or "").strip()

        created = _pick_first_present(member, ["created", "createdAt", "createdOn", "createdDate", "createDate"])
        updated = _pick_first_present(member, ["updated", "updatedAt", "updatedOn", "updatedDate", "updateDate"])

        # å¸¸è¦‹ person æ¬„ä½ï¼ˆä¸ä¸€å®šå­˜åœ¨ï¼‰ï¼šforename/surname/email/mobile
        forename = (person.get("forename") or "").strip()
        surname = (person.get("surname") or "").strip()
        email = (person.get("emailAddress") or "").strip()
        mobile = (person.get("mobileNumber") or "").strip()

        if display_name and member_id:
            rows.append({
                "æœå°‹å§“å": search_name,
                "displayName": display_name,
                "memberId": member_id,
                "passStatus": pass_status,
                "created": created,
                "updated": updated,
                "forename": forename,
                "surname": surname,
                "emailAddress": email,
                "mobileNumber": mobile,
            })

        if len(rows) >= max_hits:
            break
    return rows

def search_by_display_name(name: str, max_hits: int, operator: str) -> list[dict]:
    filters = {
        "limit": min(int(max_hits), 1000),
        "offset": 0,
        "filterGroups": [{
            "condition": "AND",
            "fieldFilters": [{
                "filterField": "displayName",
                "filterValue": name,
                "filterOperator": operator,  # "eq" or "like"
            }]
        }]
    }
    items = post_list_members(filters)
    return extract_member_rows(items, name, max_hits=max_hits)

def list_recycle_pool_issued_blank(limit: int = 200, offset: int = 0) -> list[dict]:
    """
    å›æ”¶æ± ï¼ˆè¼ƒå®‰å…¨ï¼‰ï¼šPASS_ISSUED + displayName == ""ï¼ˆç©ºç™½ä½”ä½æœƒå“¡ï¼‰
    æ³¨æ„ï¼šæ˜¯å¦æ”¯æ´ç”¨ displayName eq "" ç”± PassKit å¾Œç«¯æ±ºå®šï¼›è‹¥ä½ ç’°å¢ƒä¸åƒç©ºå­—ä¸²ï¼Œæœƒå›å‚³ 0 ç­†ã€‚
    """
    filters = {
        "limit": min(int(limit), 1000),
        "offset": int(offset),
        "filterGroups": [{
            "condition": "AND",
            "fieldFilters": [
                {"filterField": "passStatus", "filterValue": "PASS_ISSUED", "filterOperator": "eq"},
                {"filterField": "displayName", "filterValue": "", "filterOperator": "eq"},
            ]
        }],
        # è‹¥å¾Œç«¯æ”¯æ´æ’åºï¼Œé€™è£¡å¯ä»¥è®“å›æ”¶æ± æ›´ç©©å®šï¼šå…ˆç”¨è¼ƒèˆŠçš„
        "orderBy": "created",
        "orderAsc": True,
    }
    items = post_list_members(filters)

    rows: list[dict] = []
    for item in items:
        member = extract_member_obj(item)
        if not member:
            continue
        person = member.get("person") or {}
        member_id = (member.get("id") or "").strip()
        pass_status = (member.get("passStatus") or "").strip()
        display_name = (person.get("displayName") or "").strip()
        # äºŒæ¬¡ç¢ºä¿çœŸç©ºç™½
        if pass_status == "PASS_ISSUED" and member_id and display_name == "":
            rows.append({
                "memberId": member_id,
                "passStatus": pass_status,
                "created": _pick_first_present(member, ["created", "createdAt", "createdOn", "createdDate", "createDate"]),
                "updated": _pick_first_present(member, ["updated", "updatedAt", "updatedOn", "updatedDate", "updateDate"]),
            })
    return rows

def is_candidate_minimal(row: dict) -> bool:
    """
    ç”¨æ–¼ã€ŒåŒåå›æ”¶ï¼ˆé«˜é¢¨éšªï¼‰ã€çš„ä¿å®ˆç¯©é¸ï¼š
    - passStatus å¿…é ˆ PASS_ISSUED
    - person å…¶ä»–å¸¸è¦‹æ¬„ä½éƒ½ç©ºï¼ˆé¿å…æŠŠèˆŠäººçš„ email/phone ç•™è‘—ï¼‰
    ä½ å¯ä»¥ä¾ä½ å¯¦éš› schema å†åŠ æ›´åš´æ ¼æ¢ä»¶ã€‚
    """
    if (row.get("passStatus") or "") != "PASS_ISSUED":
        return False
    if (row.get("emailAddress") or "").strip():
        return False
    if (row.get("mobileNumber") or "").strip():
        return False
    if (row.get("forename") or "").strip():
        return False
    if (row.get("surname") or "").strip():
        return False
    return True

def choose_duplicate_recycle_candidates(df_hits: pd.DataFrame) -> pd.DataFrame:
    """
    å¾åŒåå¤šç­†ä¸­æŒ‘å›æ”¶å€™é¸ï¼ˆé«˜é¢¨éšªï¼‰ï¼š
    - æ¯å€‹ displayName ä¿ç•™ 1 ç­†ï¼ˆç›¡é‡ä»¥ created/updated åˆ¤æ–·æœ€æ–°ï¼›è‹¥ç„¡ï¼Œä¿ç•™æœ€å¾Œä¸€ç­†ï¼‰
    - å…¶é¤˜è‹¥ç¬¦åˆ is_candidate_minimal() å‰‡åˆ—å…¥å›æ”¶æ± 
    """
    if df_hits.empty:
        return df_hits.iloc[0:0].copy()

    work = df_hits.copy()

    # å˜—è©¦æŠŠ created/updated è½‰æˆå¯æ’åºçš„æ™‚é–“ï¼ˆå¤±æ•—å°±ä¿æŒ NaTï¼‰
    for col in ["created", "updated"]:
        if col in work.columns:
            work[col] = pd.to_datetime(work[col], errors="coerce")

    candidates = []

    for name, g in work.groupby("displayName", dropna=False):
        if len(g) <= 1:
            continue

        # å„ªå…ˆç”¨ updatedï¼Œå…¶æ¬¡ createdï¼›å¦‚æœéƒ½æ²’æœ‰ï¼Œå°±ç”¨åŸé †åº
        if g["updated"].notna().any():
            g_sorted = g.sort_values(["updated", "created"], ascending=[False, False], na_position="last")
        elif g["created"].notna().any():
            g_sorted = g.sort_values(["created"], ascending=[False], na_position="last")
        else:
            g_sorted = g.copy()

        # ä¿ç•™ç¬¬ä¸€ç­†ï¼ˆè¦–ç‚ºæœ€æ–°/ä¸»ç­†ï¼‰
        keep = g_sorted.iloc[0]
        rest = g_sorted.iloc[1:]

        for _, r in rest.iterrows():
            row_dict = r.to_dict()
            if is_candidate_minimal(row_dict):
                candidates.append(row_dict)

    if not candidates:
        return work.iloc[0:0].copy()

    return pd.DataFrame(candidates)

# ----------------------------
# UI - Input
# ----------------------------
with st.form("search_form"):
    input_text = st.text_area(
        "æ¯è¡Œä¸€å€‹ displayNameï¼ˆä½ å®šç¾©ï¼šå…¨å¤§å¯« + ç„¡ç©ºæ ¼ï¼‰â€” æœ€å¤š 150 è¡Œ",
        height=220,
        placeholder="HSIUTINGCHOU\nKUANYENLEE\nMEIHUALEE\n..."
    )

    colA, colB, colC, colD = st.columns([1, 1, 1, 2])
    with colA:
        max_hits = st.number_input("åŒåæœ€å¤šå›å‚³ç­†æ•¸", min_value=1, max_value=150, value=10, step=1)
    with colB:
        operator = st.selectbox("æ¯”å°æ–¹å¼", options=["eq", "like"], index=0)
    with colC:
        throttle = st.number_input("æ¯æ¬¡ API é–“éš”ç§’æ•¸", min_value=0.0, max_value=2.0, value=0.15, step=0.05)
    with colD:
        st.caption("eq = å®Œå…¨ç›¸åŒï¼›like = åŒ…å«ï¼ˆè¼ƒé¬†ï¼Œå¯èƒ½å›æ›´å¤šçµæœï¼‰ã€‚å»ºè­°ç”¨ eqã€‚")

    submitted = st.form_submit_button("Search")

# ----------------------------
# Run search
# ----------------------------
if submitted:
    raw_names = [n for n in (input_text or "").splitlines() if n.strip()]
    names = [normalize_name(n) for n in raw_names if normalize_name(n)]

    if not names:
        st.warning("è«‹å…ˆè²¼ä¸Šè‡³å°‘ä¸€è¡Œå§“åã€‚")
        st.stop()

    if len(names) > 150:
        st.warning(f"ä½ è²¼äº† {len(names)} è¡Œï¼Œç³»çµ±åªæœƒå–å‰ 150 è¡Œã€‚")
        names = names[:150]

    all_rows: list[dict] = []
    missing: list[str] = []

    prog = st.progress(0.0)
    status = st.empty()

    for i, name in enumerate(names, start=1):
        status.info(f"æŸ¥è©¢ä¸­ {i}/{len(names)}ï¼š{name}")
        try:
            rows = search_by_display_name(name, max_hits=int(max_hits), operator=operator)
            if rows:
                all_rows.extend(rows)
            else:
                missing.append(name)
        except Exception as e:
            st.error(f"âŒ æŸ¥è©¢å¤±æ•—ï¼š{name} â†’ {e}")
            missing.append(name)

        prog.progress(i / len(names))
        if float(throttle) > 0:
            time.sleep(float(throttle))

    status.empty()
    prog.empty()

    st.session_state["missing_names"] = missing
    st.session_state["hits_rows"] = all_rows

    st.success(f"å®Œæˆï¼šæŸ¥è©¢ {len(names)} ç­†ï¼Œå‘½ä¸­ {len(all_rows)} ç­†ï¼›æœªæ‰¾åˆ° {len(missing)} ç­†ã€‚")

# ----------------------------
# Render results
# ----------------------------
hits_rows = st.session_state.get("hits_rows") or []
missing_names = st.session_state.get("missing_names") or []

if hits_rows:
    df_hits = pd.DataFrame(hits_rows)
    # è¼ƒå¥½çœ‹çš„æ¬„ä½é †åº
    cols_order = [c for c in [
        "æœå°‹å§“å", "displayName", "memberId", "passStatus", "created", "updated",
        "forename", "surname", "emailAddress", "mobileNumber"
    ] if c in df_hits.columns]
    df_hits = df_hits[cols_order].copy()

    left, right = st.columns([2, 1], gap="large")
    with left:
        st.subheader("å‘½ä¸­æ¸…å–®")
        st.dataframe(df_hits, use_container_width=True, height=420)
        csv = df_hits.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ä¸‹è¼‰å‘½ä¸­ CSV", data=csv, file_name="passkit_member_hits.csv", mime="text/csv")

    with right:
        st.subheader("é‡è¤‡çµ±è¨ˆ")
        dup_counts = (
            df_hits.groupby("displayName")["memberId"]
            .nunique()
            .reset_index(name="åŒå memberId æ•¸é‡")
            .sort_values("åŒå memberId æ•¸é‡", ascending=False)
        )
        dup_only = dup_counts[dup_counts["åŒå memberId æ•¸é‡"] > 1].copy()
        st.metric("åŒåé‡è¤‡åç¨±æ•¸", int(len(dup_only)))
        st.dataframe(dup_only, use_container_width=True, height=260)

        if missing_names:
            st.subheader("æœªæ‰¾åˆ°åå–®ï¼ˆmissingï¼‰")
            st.write("\n".join(missing_names))
        else:
            st.info("æ²’æœ‰ missingã€‚")

elif submitted:
    st.info("æ²’æœ‰å‘½ä¸­è³‡æ–™ï¼ˆhits ç‚º 0ï¼‰ã€‚è‹¥ä½ ç¢ºèªè³‡æ–™å­˜åœ¨ï¼Œè«‹æª¢æŸ¥ PROGRAM_ID / API Prefix / operatorã€‚")

# ----------------------------
# Recycle & assign
# ----------------------------
st.divider()
st.header("â™»ï¸ å›æ”¶æ±  â†’ åˆ†é…çµ¦ missing")

if not missing_names:
    st.info("ç›®å‰æ²’æœ‰ missing åå–®ï¼Œå› æ­¤ä¸éœ€è¦åˆ†é…å›æ”¶æ± ã€‚")
else:
    st.warning(
        "âš ï¸ é‡è¦ï¼šä½ ç›®å‰æ²’æœ‰ã€Pass URL æ˜¯å¦æ›¾ç™¼é€/å¤–æµã€çš„ç´€éŒ„ã€‚\n\n"
        "PASS_ISSUED ä»£è¡¨ URL å·²å­˜åœ¨ï¼›å³ä½¿æœª installedï¼Œè‹¥ URL æ›¾è¢«ä»»ä½•äººæ‹¿åˆ°ï¼Œ"
        "ä½ æŠŠ memberId æ”¹åçµ¦åˆ¥äººï¼Œæœªä¾†æ‰“é–‹èˆŠ URL æœƒçœ‹åˆ°æ–°è³‡æ–™ï¼ˆç­‰æ–¼è½‰æ‰‹ï¼‰ã€‚\n\n"
        "å› æ­¤æˆ‘æŠŠã€åŒåå›æ”¶ã€è¨­ç‚ºé«˜é¢¨éšªé¸é …ï¼Œä¸¦é è¨­æ¨è–¦ã€ç©ºç™½ ISSUED å›æ”¶æ± ã€ã€‚"
    )

    mode = st.radio(
        "é¸æ“‡å›æ”¶æ± ä¾†æº",
        options=[
            "A) PASS_ISSUED + ç©ºç™½è³‡æ–™ï¼ˆdisplayName ç‚ºç©ºï¼‰ã€è¼ƒå®‰å…¨ / æ¨è–¦ã€‘",
            "B) åŒåé‡è¤‡ä¸­å›æ”¶èˆŠ memberIdï¼ˆé«˜é¢¨éšªï¼‰",
        ],
        index=0
    )

    col1, col2, col3 = st.columns([1, 1, 2])
    with col1:
        assign_limit = st.number_input("æœ€å¤šåˆ†é…ç­†æ•¸", min_value=1, max_value=5000, value=min(300, len(missing_names)), step=10)
    with col2:
        apply_throttle = st.number_input("æ¯æ¬¡ PUT é–“éš”ç§’æ•¸", min_value=0.0, max_value=2.0, value=0.2, step=0.05)
    with col3:
        st.caption("å»ºè­°å…ˆåš Dry-run é è¦½ mappingï¼Œç¢ºèªå¾Œå† Applyã€‚")

    recycle_pool: list[dict] = []
    pool_df: pd.DataFrame | None = None

    if mode.startswith("A)"):
        st.subheader("A) å–å›æ”¶æ± ï¼šPASS_ISSUED + displayName == ''")
        pool_limit = st.number_input("å›æ”¶æ± æ’ˆå–ä¸Šé™ï¼ˆæ¯æ¬¡ï¼‰", min_value=10, max_value=1000, value=300, step=50)
        fetch_pool = st.button("å–å¾—å›æ”¶æ± ï¼ˆAï¼‰", type="secondary")

        if fetch_pool:
            try:
                recycle_pool = list_recycle_pool_issued_blank(limit=int(pool_limit), offset=0)
                pool_df = pd.DataFrame(recycle_pool) if recycle_pool else pd.DataFrame(columns=["memberId", "passStatus", "created", "updated"])
                st.session_state["recycle_pool"] = recycle_pool
                st.success(f"å›æ”¶æ± å–å¾—å®Œæˆï¼š{len(recycle_pool)} ç­†ã€‚")
            except Exception as e:
                st.error(f"âŒ å–å¾—å›æ”¶æ± å¤±æ•—ï¼š{e}")

        recycle_pool = st.session_state.get("recycle_pool") or []
        if recycle_pool:
            pool_df = pd.DataFrame(recycle_pool)
            st.dataframe(pool_df, use_container_width=True, height=240)
        else:
            st.info("å°šæœªå–å¾—å›æ”¶æ± ï¼Œæˆ–å›æ”¶æ± ç‚ºç©ºã€‚")

    else:
        st.subheader("B) å¾åŒåé‡è¤‡ä¸­æŒ‘å›æ”¶å€™é¸ï¼ˆé«˜é¢¨éšªï¼‰")
        st.caption("è¦å‰‡ï¼ˆä¿å®ˆï¼‰ï¼šåªæŒ‘ PASS_ISSUED ä¸” person å…¶ä»–æ¬„ä½ï¼ˆemail/mobile/forename/surnameï¼‰éƒ½ç©ºçš„èˆŠç­†ã€‚")
        if hits_rows:
            df_hits = pd.DataFrame(hits_rows)
            cand_df = choose_duplicate_recycle_candidates(df_hits)
            if cand_df.empty:
                st.info("æ‰¾ä¸åˆ°ç¬¦åˆã€ä¿å®ˆæ¢ä»¶ã€çš„åŒåå›æ”¶å€™é¸ã€‚ä½ å¯ä»¥å…ˆç”¨ A æ¨¡å¼ï¼Œæˆ–èª¿æ•´å€™é¸æ¢ä»¶ã€‚")
            else:
                st.success(f"æ‰¾åˆ°åŒåå›æ”¶å€™é¸ï¼š{len(cand_df)} ç­†ã€‚")
                st.dataframe(
                    cand_df[["displayName", "memberId", "passStatus", "created", "updated", "emailAddress", "mobileNumber"]],
                    use_container_width=True,
                    height=260
                )
                recycle_pool = [{"memberId": x} for x in cand_df["memberId"].tolist() if str(x).strip()]
                st.session_state["recycle_pool_dup"] = recycle_pool
        else:
            st.info("ä½ éœ€è¦å…ˆ Search å–å¾—å‘½ä¸­è³‡æ–™ï¼Œæ‰èƒ½ç”¨åŒåå›æ”¶ï¼ˆBï¼‰ã€‚")

        if mode.startswith("B)") and st.session_state.get("recycle_pool_dup"):
            recycle_pool = st.session_state["recycle_pool_dup"]

    # Mapping preview
    st.subheader("ğŸ“Œ Dry-runï¼šç”¢ç”Ÿåˆ†é… mappingï¼ˆä¸æœƒå¯«å…¥ï¼‰")

    # æ± å­åªæ‹¿ memberId
    pool_ids = []
    for x in recycle_pool or []:
        mid = (x.get("memberId") or "").strip()
        if mid:
            pool_ids.append(mid)

    # å»é‡ï¼ˆä¿æŒé †åºï¼‰
    seen = set()
    pool_ids_unique = []
    for mid in pool_ids:
        if mid not in seen:
            pool_ids_unique.append(mid)
            seen.add(mid)

    max_assign = int(min(assign_limit, len(missing_names), len(pool_ids_unique)))
    if max_assign <= 0:
        st.info("ç›®å‰ç„¡æ³•ç”¢ç”Ÿ mappingï¼šå¯èƒ½æ˜¯å›æ”¶æ± ç‚ºç©ºï¼Œæˆ– missing ç‚ºç©ºã€‚")
    else:
        mapping = []
        for i in range(max_assign):
            mapping.append({
                "new_displayName": missing_names[i],
                "recycled_memberId": pool_ids_unique[i],
            })
        df_map = pd.DataFrame(mapping)
        st.dataframe(df_map, use_container_width=True, height=260)

        map_csv = df_map.to_csv(index=False).encode("utf-8-sig")
        st.download_button("ä¸‹è¼‰ mapping CSVï¼ˆDry-runï¼‰", data=map_csv, file_name="recycle_mapping_dryrun.csv", mime="text/csv")

        st.divider()
        st.subheader("âœ… Applyï¼šä¾ mapping æ‰¹æ¬¡æ›´æ–°ï¼ˆPUTï¼‰")

        ack = st.checkbox(
            "æˆ‘äº†è§£é¢¨éšªï¼šPASS_ISSUED ä»å¯èƒ½å·²è¢«åˆ†äº«ã€‚è‹¥å›æ”¶çš„ memberId/URL æ›¾å¤–æµï¼Œæ›´æ–°å¾Œæœƒè®“èˆŠ URL æŒ‡å‘æ–°æœƒå“¡è³‡æ–™ï¼ˆç­‰æ–¼è½‰æ‰‹ï¼‰ã€‚",
            value=False
        )
        do_apply = st.button("Apply æ‰¹æ¬¡æ›´æ–°", type="primary", disabled=not ack)

        if do_apply:
            ok_rows = []
            fail_rows = []

            prog2 = st.progress(0.0)
            status2 = st.empty()

            for i, row in enumerate(mapping, start=1):
                new_name = row["new_displayName"]
                member_id = row["recycled_memberId"]

                status2.info(f"æ›´æ–°ä¸­ {i}/{len(mapping)}ï¼š{member_id} â†’ {new_name}")

                try:
                    payload = {
                        "programId": PROGRAM_ID,
                        "id": member_id,
                        "person": {
                            "displayName": new_name
                        }
                    }
                    resp = put_update_member(member_id, payload)
                    ok_rows.append({
                        "memberId": member_id,
                        "new_displayName": new_name,
                        "result": "OK",
                        "resp": str(resp)[:500],
                    })
                except Exception as e:
                    fail_rows.append({
                        "memberId": member_id,
                        "new_displayName": new_name,
                        "result": "FAIL",
                        "error": str(e)[:800],
                    })

                prog2.progress(i / len(mapping))
                if float(apply_throttle) > 0:
                    time.sleep(float(apply_throttle))

            status2.empty()
            prog2.empty()

            st.success(f"å®Œæˆï¼šæˆåŠŸ {len(ok_rows)} ç­†ï¼›å¤±æ•— {len(fail_rows)} ç­†ã€‚")

            if ok_rows:
                df_ok = pd.DataFrame(ok_rows)
                st.subheader("æˆåŠŸæ¸…å–®")
                st.dataframe(df_ok, use_container_width=True, height=260)
                ok_csv = df_ok.to_csv(index=False).encode("utf-8-sig")
                st.download_button("ä¸‹è¼‰æˆåŠŸ CSV", data=ok_csv, file_name="recycle_apply_success.csv", mime="text/csv")

            if fail_rows:
                df_fail = pd.DataFrame(fail_rows)
                st.subheader("å¤±æ•—æ¸…å–®ï¼ˆè«‹é‡è©¦æˆ–æª¢æŸ¥ API/è³‡æ–™ï¼‰")
                st.dataframe(df_fail, use_container_width=True, height=260)
                fail_csv = df_fail.to_csv(index=False).encode("utf-8-sig")
                st.download_button("ä¸‹è¼‰å¤±æ•— CSV", data=fail_csv, file_name="recycle_apply_failed.csv", mime="text/csv")
